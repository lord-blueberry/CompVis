{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-cb57480307ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# for displaying images in jupyter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# install all needed packages\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import skimage\n",
    "from skimage import transform\n",
    "import skimage.data\n",
    "import skimage.io\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# for displaying images in jupyter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_binary_test_image_classes(height: int, width: int, nr_classes)->(np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Generate a test image and an appropriate label image with label 0 or label 1\n",
    "    Label 0 images have a lower medium grayscale value, label 1 images a higher one.\n",
    "    \n",
    "    The returned images will have shape (height, width, 1) and type uint8 to be as compatible as possible to \n",
    "    mages read from files\n",
    "    \"\"\" \n",
    "    class_id = np.random.choice(range(nr_classes))\n",
    "    value = 255.0 * class_id / nr_classes \n",
    "    \n",
    "    NOISE = 20.0\n",
    "    \n",
    "    label = np.zeros((height, width, 1), dtype=np.uint8)\n",
    "    label.fill(class_id)\n",
    "    img = np.random.normal(value, NOISE, ((height, width, 1))).astype(np.uint8)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "def gen_batch_binary_with_border(batch_size, height: int, width: int, border: int, nr_classes:int):\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for i in range(batch_size):\n",
    "        img, label = gen_random_binary_test_image_classes(height=height, width=width, nr_classes=nr_classes)\n",
    "        if(border > 0):\n",
    "            label = label[border:-border, border:-border]\n",
    "        image_list.append(img.astype(np.float32) / 255.0)\n",
    "        label_list.append(label.astype(np.float32))\n",
    "\n",
    "    image_batch = np.array(image_list, dtype=np.float32)\n",
    "    label_batch = np.array(label_list, dtype=np.float32)\n",
    "\n",
    "    # reshape labels as this is not done in the model\n",
    "    label_batch = label_batch.reshape(batch_size, (height-2*border)*(width-2*border), 1)\n",
    "    label_batch = keras.utils.to_categorical(label_batch, num_classes=nr_classes)\n",
    "\n",
    "    return image_batch, label_batch\n",
    "\n",
    "def batch_generator_with_border(batch_size, height: int, width: int, border: int, nr_classes:int):\n",
    "    while True:\n",
    "        image_batch, label_batch = gen_batch_binary_with_border(batch_size=batch_size, \n",
    "                                                                height=height, width=width, \n",
    "                                                                border=border, nr_classes=nr_classes)\n",
    "        yield image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = gen_batch_binary_with_border(100, 128, 128, 3, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading from dtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES_DTD = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(folder, imgIdx: int, height: int, width: int):\n",
    "    imgPath = os.path.join(folder, \"image_{:05d}.png\".format(imgIdx))\n",
    "    labelPath = os.path.join(folder, \"label_{:05d}.png\".format(imgIdx))\n",
    "    \n",
    "    img = skimage.io.imread(imgPath, as_gray=True)\n",
    "    img =  transform.resize(img, (height, width), anti_aliasing=True).reshape((height, width,1))\n",
    "    img = (img - img.mean()) \n",
    "    img = img / np.sqrt(img.var())\n",
    "    label = skimage.io.imread(labelPath, as_gray=True)\n",
    "    label =  transform.resize(label, (height, width), anti_aliasing=True).reshape((height, width,1))\n",
    "    return img, label\n",
    "\n",
    "def load_image2(folder, imgIdx: int, height: int, width: int):\n",
    "    imgPath = os.path.join(folder, \"image_{:05d}.png\".format(imgIdx))\n",
    "    labelPath = os.path.join(folder, \"label_{:05d}.png\".format(imgIdx))\n",
    "    \n",
    "    img = skimage.io.imread(imgPath, as_gray=True)\n",
    "    img =  transform.resize(img, (height, width), anti_aliasing=True).reshape((height, width,1))\n",
    "    label = skimage.io.imread(labelPath, as_gray=True)\n",
    "    label =  transform.resize(label, (height, width), anti_aliasing=True)[0, 0]\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(folder, nr_classes, indices, height: int, width: int, border:int):\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for i in indices:\n",
    "        img, lbl = load_image(folder, i, height, width)\n",
    "        if(border > 0):\n",
    "            lbl = lbl[border:-border, border:-border]\n",
    "        image_list.append(img)\n",
    "        label_list.append(lbl)\n",
    "    \n",
    "    image_batch = np.array(image_list, dtype=np.float32)\n",
    "    label_batch = np.array(label_list, dtype=np.float32)\n",
    "\n",
    "    # reshape labels as this is not done in the model\n",
    "    label_batch = label_batch.reshape(len(indices), (height-2*border)*(width-2*border), 1)*255\n",
    "    label_batch = keras.utils.to_categorical(label_batch, num_classes=nr_classes)\n",
    "    return image_batch, label_batch\n",
    "\n",
    "def load_batch2(folder, indices, height: int, width: int):\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for i in indices:\n",
    "        img, lbl = load_image2(folder, i, height, width)\n",
    "        image_list.append(img)\n",
    "        label_list.append(lbl)\n",
    "    \n",
    "    image_batch = np.array(image_list, dtype=np.float32)\n",
    "    label_batch = np.array(label_list, dtype=np.float32)\n",
    "    \n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_random_images_generator(batch_size, folder, nr_classes, height: int, width: int, border:int):\n",
    "    indices = list(range(0, 1840))\n",
    "    random.shuffle(indices)\n",
    "    i = 0\n",
    "    while True:\n",
    "        ind = indices[i:i+batch_size]\n",
    "        image_batch, label_batch = load_batch(folder, nr_classes, ind, height=height, width=width, border=border)\n",
    "        i = i + batch_size\n",
    "        i = i % len(indices)\n",
    "        yield image_batch, label_batch\n",
    "\n",
    "def load_batch_random_images_generator2(batch_size, folder, height: int, width: int):\n",
    "    indices = list(range(0, 1840))\n",
    "    random.shuffle(indices)\n",
    "    i = 0\n",
    "    while True:\n",
    "        ind = indices[i:i+batch_size]\n",
    "        image_batch, label_batch = load_batch2(folder, ind, height=height, width=width)\n",
    "        i = i + batch_size\n",
    "        i = i % len(indices)\n",
    "        yield image_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_simple_border(input_height:int, input_width:int, nr_classes:int) -> (keras.Model, int):\n",
    "    \"\"\"\n",
    "    Create a simple fcn model for semantic segmentation with 2 classes.\n",
    "    Return both the model and the border size\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    border = 0;\n",
    "    \n",
    "    # we use grayscale (1-channel input)\n",
    "    \n",
    "    # (used to define input shape on the first layers)\n",
    "    model.add(keras.layers.Layer(input_shape=(input_height , input_width, 1)))\n",
    "    \n",
    "    # add 3 convolutional layers with 3x3 filters\n",
    "    model.add(keras.layers.Convolution2D(filters=4, kernel_size=3, padding='valid', activation='relu'))\n",
    "    border = border + 1\n",
    "    model.add(keras.layers.Convolution2D(filters=8, kernel_size=3, padding='valid', activation='relu'))\n",
    "    border = border + 1\n",
    "    model.add(keras.layers.Convolution2D(filters=4, kernel_size=3, padding='valid', activation='relu'))\n",
    "    border = border + 1\n",
    "    \n",
    "    # go to logits which is the number of classes and add sigmoid layer for activation\n",
    "    model.add(keras.layers.Convolution2D(filters=nr_classes, kernel_size=1, activation=None, \n",
    "                                         kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.001, seed=None)))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "    \n",
    "    # reshape so that we have a sample for each pixel\n",
    "    model.add(keras.layers.Reshape(target_shape=((input_height-2*border) * (input_width-2*border), nr_classes)))\n",
    "    \n",
    "    return model, border"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_more_comlex_border(input_height:int, input_width:int, nr_classes:int) -> (keras.Model, int):\n",
    "    border = 0;\n",
    "    # we use grayscale (1-channel input)\n",
    "    \n",
    "    # (used to define input shape on the first layers)\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Layer(input_shape=(input_height , input_width, 1)))\n",
    "    \n",
    "\n",
    "    model.add(keras.layers.Convolution2D(filters=8, kernel_size=3, use_bias=True, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.Convolution2D(filters=8, kernel_size=3, use_bias=True, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(strides=2, padding='same')) \n",
    "    \n",
    "    model.add(keras.layers.Convolution2D(filters=12, kernel_size=3, use_bias=True, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.Convolution2D(filters=12, kernel_size=3, use_bias=True, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(strides=2, padding='same'))\n",
    "    \n",
    "    model.add(keras.layers.Convolution2D(filters=18, kernel_size=3, use_bias=True, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.Convolution2D(filters=18, kernel_size=3, use_bias=True, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(strides=2, padding='same')) \n",
    "    \n",
    "    # go to logits which is the number of classes and add sigmoid layer for activation\n",
    "    model.add(keras.layers.Convolution2D(filters=nr_classes, kernel_size=1, activation=None, \n",
    "                                         kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.001, seed=None)))\n",
    "    model.add(keras.layers.UpSampling2D(size=(8, 8), data_format=None, interpolation='nearest'))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "    \n",
    "    # reshape so that we have a sample for each pixel\n",
    "    model.add(keras.layers.Reshape(target_shape=((input_height-2*border) * (input_width-2*border), nr_classes)))\n",
    "    \n",
    "    return model, border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_42 (Layer)             (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_208 (Conv2D)          (None, 128, 128, 8)       80        \n",
      "_________________________________________________________________\n",
      "conv2d_209 (Conv2D)          (None, 128, 128, 8)       584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_210 (Conv2D)          (None, 64, 64, 12)        876       \n",
      "_________________________________________________________________\n",
      "conv2d_211 (Conv2D)          (None, 64, 64, 12)        1308      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 32, 32, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_212 (Conv2D)          (None, 32, 32, 18)        1962      \n",
      "_________________________________________________________________\n",
      "conv2d_213 (Conv2D)          (None, 32, 32, 18)        2934      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 16, 16, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_214 (Conv2D)          (None, 16, 16, 48)        912       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_30 (UpSampling (None, 128, 128, 48)      0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128, 128, 48)      0         \n",
      "_________________________________________________________________\n",
      "reshape_19 (Reshape)         (None, 16384, 48)         0         \n",
      "=================================================================\n",
      "Total params: 8,656\n",
      "Trainable params: 8,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 8s 420ms/step - loss: 3.8698 - acc: 0.0252\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 10s 498ms/step - loss: 3.8096 - acc: 0.0244\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.7878 - acc: 0.0323\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.7656 - acc: 0.0361\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.7435 - acc: 0.0402\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 10s 497ms/step - loss: 3.7220 - acc: 0.0417\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.7094 - acc: 0.0409\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.7032 - acc: 0.0381\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 3.7024 - acc: 0.0411\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 3.6875 - acc: 0.0425\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 10s 494ms/step - loss: 3.6753 - acc: 0.0468\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 10s 488ms/step - loss: 3.6796 - acc: 0.0444\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.6726 - acc: 0.0441\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 3.6672 - acc: 0.0464\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 3.6687 - acc: 0.0482\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 3.6699 - acc: 0.0471\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 3.6597 - acc: 0.0486\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 10s 494ms/step - loss: 3.6482 - acc: 0.0496\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 10s 492ms/step - loss: 3.6441 - acc: 0.0521\n",
      "Epoch 20/20\n",
      "15/20 [=====================>........] - ETA: 2s - loss: 3.6317 - acc: 0.0530"
     ]
    }
   ],
   "source": [
    "PATCH_HEIGHT = 128\n",
    "PATCH_WIDTH = 128\n",
    "BATCH_SIZE = 80\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCHS = 20\n",
    "NR_CLASSES = 48\n",
    "model, border = fcn_more_comlex_border(PATCH_HEIGHT, PATCH_WIDTH, NR_CLASSES)\n",
    "model.summary()\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit_generator(\n",
    "        generator=load_batch_random_images_generator(BATCH_SIZE, \"dtd_train\", NR_CLASSES, PATCH_HEIGHT, PATCH_WIDTH, border),\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        steps_per_epoch=STEPS_PER_EPOCHS,\n",
    "        shuffle=False,\n",
    "        callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 97.83%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(\n",
    "    generator=load_batch_random_images_generator(184, \"dtd_test\", NR_CLASSES, PATCH_HEIGHT, PATCH_WIDTH, border),\n",
    "    steps=10)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 97.83%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 12., 13., 14.,\n",
       "       15., 16., 17., 18., 19., 20., 21., 23., 24., 25., 26., 27., 28.,\n",
       "       30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n",
       "       43., 45., 46.], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_batch_binary_with_border(batch_size, height: int, width: int, border: int, nr_classes:int):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla, label = load_image(\"dtd_train\", 2, 16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = load_batch_random_images_generator(184, \"dtd_train\", 48, PATCH_HEIGHT, PATCH_WIDTH, border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.4064436"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[2,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.97912"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(bla, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.37730665019913934 0.41877178117340685 0.035050713205133255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "imgPath = os.path.join(\"dtd_train\", \"image_{:05d}.png\".format(2))\n",
    "\n",
    "img = skimage.io.imread(imgPath, as_gray=True)\n",
    "img =  transform.resize(img, (128, 128), anti_aliasing=True).reshape((128, 128,1))\n",
    "img = (img - img.mean()) \n",
    "print(img.min(), img.max(), img.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = [32, 0.3, 0.111, -.03, -9,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.20e+01,  3.00e-01,  1.11e-01, -3.00e-02, -9.00e+00,  3.00e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bla, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
